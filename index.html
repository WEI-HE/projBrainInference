<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Project inference : Design an experiment which requires human subjects to do inference, analyse, mine and model their brain activity to understand how the brain do inference ?">
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <!--load font families-->
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <link rel="stylesheet" href="fonts/Serif/cmun-serif.css" />
    
    <!--Mathematics with MathJax-->
    <script type="text/x-mathjax-config">
      // MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      // MathJax.Hub.Config(
      //   {
      //   tex2jax: {
      //     inlineMath: [['$','$'], ['\\(','\\)']]
      //   } 
      //   "HTML-CSS": {
      //     preferredFont: "STIX",
      //     matchFontHeight: true,
      //   }
      //   });
      MathJax.Hub.Config({
          extensions: ["tex2jax.js"],
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
          },
          "HTML-CSS": { 
            availableFonts: ["TeX"],
            matchFontHeight: true,
          }
      });
      
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
  </head>
  
  

  <body>
    
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/steevelaquitaine/projBrainInference">Github</a>
          <a id="project_author" href="http://steevelaquitaine.blogspot.com">Steeve laquitaine </a>
        </header>
    </div>

    <!--TITLE-->
    <div id="proj_title_wrap" class="outer">
        <header class="inner">
        <section id="proj_title" class="inner">
        <h1 id="proj_title"> How does the brain do visual inference ? </h1>
    </div>
    
    

    <!--Table of contents-->
    <div id="Table_of_content_wrap" class="outer">
      <section id="table_of_content" class="inner">
        <h5> TABLE OF CONTENT </h5>
        <h7><a href="#Inference"> Inference <br></h7>
        <h7><a href="#Design-an-inference-experiment"> Design an inference experiment <br></h7>
        <h7><a href="#Database"> Database <br></h7>
        <h7><a href="#Data preprocessing workflow"> Data preprocessing workflow <br></h7>
        <h7><a href="#Mapping-brain-areas"> Mapping brain areas <br></h7>
        <h7><a href="#Look-at-fMRI-voxel-responses"> Look at fmri voxel responses  <br></h7>
        <h7><a href="#Brain-decoding-of-stimulus-machine-learning"> BRAIN DECODING (FISHER LINEAR DISCRIMINANT-LOO) <br></h7>
        <h7><a href="#Motion-direction"> &nbsp &nbsp MOTION DIRECTION <br></h7>
        <h7><a href="#Motion-noise"> &nbsp &nbsp MOTION NOISE <br></h7>
        <h7><a href="#switching"> &nbsp &nbsp ESTIMATION BEHAVIOR <br></h7>
        <h7><a href="#voxel-population-analysis"> &nbsp &nbsp BRAIN RESPONSE POPULATION ANALYSIS <br></h7>
        <h7><a href="#Brain-decoding-with-Channel-Encoding-reconstruction"> &nbsp &nbsp BRAIN DECODING WITH FORWARD MODELING RECONSTRUCTION <br></h7>
        <h7><a href="#Probabilistic-population-decoding"> &nbsp &nbsp PROBABILISTIC POPULATION DECODING <br></h7>
        </section>
    </div>
    
    
    
    
    
    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
      <h6><a id="Inference" class="anchor" href="#Inference" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Inference</h3>  
      <p></p>

      <h6><a id="Design-an-inference-experiment" class="anchor" href="#Design-an-inference-experiment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Designing an inference experiment</h6>
      <p>We designed a motion direction estimation experiment in which humans were asked to estimate the 
      motion direction of noisy stimuli on a computer screen. In this experiment statistical optimality 
      can be achieved by combining noisy evidence of the motion with knowledge of the motion direction 
      statistics learnt over motion stimulus history using Bayesian inference</p>
      
      <p> Subjects were first trained with one prior distribution then scanned with the same prior. 
      In subsequent session, subjects are trained with a new prior and scanned with this new prior. </p>
      <center><img src="images/experiment.png"></center>
      
      <p> The line by line steps of the experiment are stored in the following matlab script : <p>
      <!--Codes-->
      <code class="code">
          >> run runAlltaskDirfMRI05.m
      </code>
      
      <h6><a id="Database" class="anchor" href="#Database" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Database</h3>
      <a href="dataset/datasetBrainInfer.csv"> Click for dataset info </a>

      <h6><a id="Data-preprocessing-workflow" class="anchor" href="#Data-preprocessing-workflow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data preprocessing workflow</h3>
      <p> The raw behavioral data we collected are saved in files under matlab format (".mat") in a project folder we called "sltaskDotDirfMRI05".
      Open matlab script "slworkflowBehData.m". It contains the steps to preprocess and organize the data in ".mat" file format ready  
      to be analyzed with the script "analyses.m". The subject id and its associated files for each condition are already set in "slworkflowBehData.m"  </p>
      
      <!--code-->
      <code class="code">
          >> run slworkflowBehData.m <br>
          >> analyses({'sub02'},{'StimStrength','Pstd','FeatureSample'},'inpath','experiment','vonMisesPrior','dataDis','signDistance');<br>
      </code>
      
      
      <h6><a id="Mapping-brain-areas" class="anchor" href="#Mapping-brain-areas" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mapping brain areas</h3>
      <p> The predominant view is that the brain is a collection of areas that are specialized for particular processing. e.g., The occipital cortex (largely involved in visual perception) has been divided into areas included V1, specialized in edge orientation processing, MT, specialized in motion processing etc...
      The first step consists in mapping those visual areas. We used retinotopic mapping to identify visual areas : 6-8 scans of bars swipping the visual screen.
      We then run a population receptive field analysis that identify voxels preferred spatial location (eccentricity and polar angle).
      The analysis consists in modeling voxels responses to different stimulus locations with a 2D Gaussian that accounts for the hemodynamic response
      timecourse, hemodynamic response timecourse was modelled with a difference of gamma function (fminsearch). <p>
        
      <p>We roughly divided the brain into parietal and occipital regions. To visualize those areas in mrTools load a surface (.off fiels) 
      produced by freesurfer, create an occipital/parietal flat map on that surface, load the flat map and create an roi
      out of the entire flat map, then display the roi on the surface). </p>

      <center><img src="images/left_parietal_occipital.png" style="width: 50%; height: 50%"/></center>

      
      
      <p> We then run an event-related analysis to map the areas that responded to motion (highlighted below in hot colors (yellow) on different views of an inflated brain </p>
      <center><img src="images/MotionBrain.png" style="width: 50%; height: 50%"/></center>
      
      <h6><a id="Look-at-fMRI-voxel-responses" class="anchor" href="#Look-at-fMRI-voxel-responses" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Look at fMRI voxel responses</h3>
          
      <!--code-->
      <code class="code">   
        <p>  >> slfmriInitAnalysisTaskDotDirfMRI05 <br>
          >> [d,o,behbySess] = slfmriGetDBoverSessions(o,varargin); <br>
          >> [~,VoxelsParams] = slfmriGetVoxTuningParams('vonMisesprior','neuralvector',d,o); <br>
          >> slfmriVisualDatabase(d.instances(d.mySwitch==1,:),'x1',d.myRandomDir(d.mySwitch==1,:),'x2',VoxelsParams.modes1); <br>
          >> slfmriVisualDatabase(d.instances(d.mySwitch==2,:),'x1',d.myRandomDir(d.mySwitch==2,:),'x2',VoxelsParams.modes2) </p>
      </code>

      
      <h6><a id="Decode-stimulus-features-from-brain-activity" class="anchor" href="#Decode-stimulus-features-from-brain-activity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decode stimulus features from brain activity</h3>  
      
      <ul>
      <li> Are we able to decode prior conditions from any area? </li>
      <li> Are we able to decode likelihood conditions (motion directions, coherence) ? </li>
      </ul>
      
      <h6><a id="Decode-direction-with-machine-learning" class="anchor" href="#Decode-direction-with-machine-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decode direction with machine learning</h3>  
      <p>We decoded the 5 motion directions from the voxels Bold pattern measured in 8 visual brain regions engaged in 
      processing motion information. </p>
      
      <!--Code-->
      <code class="code">
        
          <p>
          <p> %Initialize parameters <br>
            >> o = slfmriInitTask('Concatenation',1,2,1,2,o); <br>
            >> o = slfmriInitSession('~/data/datafMRI/sltaskdotdirfmri05/',... <br>
            &nbsp &nbsp &nbsp &nbsp    {'s02520150814','s02520150923','s02520150925'},o); <br>
            >> o = slfmriInitRois({'outsideBrain03','V1','V2','V3','V3A','MT','IPS','V1toMT'},...<br>
            &nbsp &nbsp &nbsp &nbsp   's0025_flatL_WM_occipital_Rad90',... <br>
            &nbsp &nbsp &nbsp &nbsp  '~/data/datafMRI/mlrAnatDB/s0025/mlrBaseAnatomies/',... <br>
            &nbsp &nbsp &nbsp &nbsp  '~/data/datafMRI/mlrAnatDB/s0025/mlrROIs/',o); <p>
          </p>
          
          <p> %plot <br>
          >> slfmriClassifyMotionDirections(o); 
          <p>

      </code>
      
      <h6><a id="Decode-coherence-with-machine-learning" class="anchor" href="#Decode-coherence-with-machine-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decode coherence with machine learning</h3>  
      <p>We also decoded the 2 coherences from the voxels Bold pattern same brain regions. </p>
      
      <!--Code-->
      <code class="code">      
           %First initialize the parameters as above for directions .. <br>
           %..then decode <br>
           >> slfmriClassifyMotionCoherence(o);
      </code>
      
      <!--Section-->
      <h6><a id="Decode-switching-with-machine-learning" class="anchor" href="#Decode-switching-with-machine-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decode switching with machine learning</h3>  
      <p>We also decoded when subjects switched to the prior or the likelihood from the voxels Bold pattern of the same brain regions. </p>
      
      <!--Code-->
      <code class="code">      
           %First initialize the parameters as above for directions .. <br>
           %..then decode <br>
           >> slfmriClassifySwitchingbyCoh(o);
      </code>

      <!--text-->
      <p><strong> Did the subject's brain maintained a representation of the displayed motion direction when the subject switched to his prior ? </strong>
      <p> We decoded the 4 motion directions (except at the prior mean, because those trials can be classified as switching to evidence or prior mean) displayed on each trial 
      from the activity patterns recorded from the subject s025's brain when he switched to his prior mean and when he switched to the 
      sensory evidence separately (at the weakest motion, 6% coherence, 3 sessions of 80 deg prior). The sample size was 32 trials/direction and 8 trials/direction when subjects 
      switched to sensory evidence and prior respectively).</p> 
      
      <!--code-->
      <code class="code">
      <p> >> slfmriInitClassifAnalysisTaskDotDirfMRI05 <br>
       >> dataClassif = slfmriClassifyDir_x_switch(o) </p>
      </code>
      
      <!--figure-->
      <center><img src="images/Classif4DirBySwitch.png" style="width: 70%; height: 70%"/></center>
      
      <p><strong> The classifier showed very poor decoding accuracies when trying to classify 5 motion directions. But it might have 
      a hard time identifying differences in patterns elicited by 5 motion directions.
      
      <!--subsection-->
      <p><strong> It is unfair to compare the classification accuracies produced from 32 trials in one condition with 
      the accuracy produced by classifying from 8 trials which should be poorer because the sample size is smaller. </strong> 
      We equalized the sample size between switching conditions to 8 instances (sampled out of 32 without 
      replacement) and rerun the classifications for 7 rois (V1 - V3, V3A, MT, IPS) and a control roi outside the brain. 
      Sampling was performed 100 times (minimum number of samples) from each class and the 100 resulting classification 
      accuracies were averaged for each conditions and roi with the 95% confidence interval calculated over bootstrapped 
      samples. </p> 
        
      <!--Code-->
      <code class="code">
        
          <p>>> slfmriInitClassifAnalysisTaskDotDirfMRI05 <br></p>

          <p>>> params = {'accuracyAtTime',[7 14; 7 14;7 14; 7 14;7 14; 7 14;7 14],... <br>
              &nbsp &nbsp &nbsp &nbsp       'loadSavedROI','CalcInstances','leaveOneOut','fisher',... <br>
              &nbsp &nbsp &nbsp &nbsp       'numInstances',8,'CalcInstancesByCond'};</p>

          <p>>> myConds = {{'myRandomDir_x_mySwitch=1_x_myRandomCoh=0.06'},.... <br>
              &nbsp &nbsp &nbsp &nbsp     {'myRandomDir_x_mySwitch=2_x_myRandomCoh=0.06'}}; <br></p>

          <p>>> [stat,cbycByROI,sbycByROI,oByROI,obycByROI]=... <br>
              &nbsp &nbsp &nbsp &nbsp     slfmriClassifyBootInstByConditions(100,params,myConds,o);</p>
              
      </code>
          
      <center><img src="images/classifAccEqSamplSizeByBoot.png" style="width: 50%; height: 50%"/></center>
          
      <!--results-->
      <p> None of the accuracies were significantly above chance. There might seem below chance because 
      Fisher leaveOneOut slightly bias predictions against the test set (not sure, it's an explanation for 
      svm but it should not bias classification for fisher) or because classification outputs are unstable for
      such small sample size (8 instances). </p>
      
      
      
      <!--subsection-->
      <p> Can he do better with 2 motion directions ? </strong> </p>
      <p> We chose the two directions furthest from the prior (225 deg in blue), where switching is the clearest: 15 and 85 degrees. 
      The classifier outputted one class at each of the 8/32 trials collected for each direction and when the subject switched to prior/evidence. 
      We report the classifier's percent correct prediction. </p> 
      <!--code-->
      <code class="code">
        <p> >> drawVectors([15 85 155 225 295],[1 1 1 1 1]) </p>
      </code>
      
      <!--figure-->
      <center><img src="images/ClassifyTwoDirections.png" style="width: 100%; height: 100%"/></center>
      
      <p> We classified the two directions from various visually responsive areas of the brain (V1,V2,V3,V3A,hMT,IPS) 
      for subject "s025" over 3 sessions when motion and the prior were weak (6% coherence, 80 deg prior). We also 
      classified the directions from signals recorded outside the brain as a control check that significant accuracies were not 
      spurious (e.g., error in the code).
      
      <!--figure-->
      <center><img src="images/ControlROI.png" style="width: 50%; height: 50%"/></center>
      
      <!--code-->
     <code class="code">
         <p> >> slfmriInitClassifAnalysisTaskDotDirfMRI05_loop; <br> 
              >> [accuracy, ste, dataClassifsw1, dataClassifsw2, nTrials] = slfmriClassifyTwoDir_x_switch_loop(o); </p>
      </code>
      
      <!--figure-->
      <center><img src="images/ClassifTwoDir.png" style="width: 50%; height: 50%"/></center>
      
      <!--SECTION-->
      <h6><a id="Analysis-of-population-of-voxel-selectivity" class="anchor" href="#Analysis-of-population-of-voxel-selectivity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>VOXEL POPULATION ANALYSIS</h3>  

      <code class="code">
        <p> >> slfMRIwrapperDisVoxSelbalDir </p>
      </code>

      <h7><a href="#Brain-decoding-with-Channel-Encoding-reconstruction"> &nbsp &nbsp BRAIN DECODING WITH FORWARD MODELING RECONSTRUCTION <br></h7>
      <h6><a id="Brain-decoding-with-Channel-Encoding-reconstruction" class="anchor" href="Brain-decoding-with-Channel-Encoding-reconstruction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>STIMULS RECONSTRUCTION WITH BRAIN RESPONSE FORWARD MODELING</h3>   


    
      <!--SECTION-->
      <h6><a id="Probabilistic-population-decoding" class="anchor" href="#Probabilistic-population-decoding" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PROBABILISTIC POPULATION DECODING</h3>   
      <p> We used a forward modeling approach (Brouwer & Heeger) that consists in modeling the brain voxel responses as a linear sum 
      of cosine functions (we could think of them as neural tuning functions to a stimulus feature) to reconstruct a stimulus feature
      (here the motion direction). </p>
        
      <!--Equations-->
      Each voxel $i$ response $b_i$ is modelled by : <br>
      
      \[ b_i = \sum_k^K W_{i_k}(f_k(s) + \eta_k) + \nu_i \]
      
      where, <br>
      $i$ : voxel i <br>
      $s$ : stimulus direction <br>
      $W_{i_{k}}$ : contribution of population $k$ to the response of voxel $i$ <br>
      $f_k(s)$ : direction tuning functions (also called channels, half-wave rectified cosines raised to the power 5) <br>
      of $K$ neural populations ($K$=8) tuned to different directions <br>
      Tuning functions are defined as : <br>
      \[f_k(s) = max \Big(0,cos(\pi(s-\Phi_k)/90) \Big))^5\]
      where, <br>
      $\Phi_k$ : orientation preference of neural population $k$ <br>
      
      $\eta_k$ and $\nu_i$ are deviations (noise) from mean response of channel $k$ and mean voxel $i$ responses respectively. <br>
      They are modelled as follows : <br>
      $\eta \sim \mathcal{N}(0,\sigma^2I)$ : noise (deviation from the mean response of a channel) shared among neural populations of similar tunings (1 x K vector) <br><br>
      $\nu \sim \mathcal{N}(0,\Sigma)$ : the linear combination of two sources of noise with covariance matrices: <br>
      - 1) $\rho\tau\tau^T$ the covariance matrix (square symmetric matrix of M x M voxels) that defines the noise specific to individual voxels $i$ which contribution to $\nu$ is scaled by $rho$ <br>
      , where $\tau$ is a 1 x M vector that models the std $\tau_i$ of each voxel $i$ noise <br>
      - 2) $(1-\rho)I\circ\tau\tau^T$ the covariance matrix (diagonal square matrix of M x M voxels) that defines the noise shared among voxels irrespective of their tunings <br>
      (common deviation from voxels mean response scaled by $1-\rho$) <br> 
      $\tau$ : a vector of 1 x M voxels that models the std $\tau_i$ of each voxel's noise <br>
      $\rho$ : factor modeling the contribution of the voxel-specific noise and the noise shared globally among voxels irrespective of their tunings <br><br>
      $\Sigma = \rho\tau\tau^T + (1-\rho)I\circ\tau\tau^T$  
          
    </div>
    
    
    
    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">projBrainInference maintained by <a href="https://github.com/steevelaquitaine">steevelaquitaine</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

  </body>
</html>
