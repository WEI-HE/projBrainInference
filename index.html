<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Project inference : Design an experiment which requires human subjects to do inference, analyse, mine and model their brain activity to understand how the brain do inference ?">
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <!--load font families-->
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <link rel="stylesheet" href="fonts/Serif/cmun-serif.css" />
    
    <!--Mathematics with MathJax-->
    <script type="text/x-mathjax-config">
      // MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      // MathJax.Hub.Config(
      //   {
      //   tex2jax: {
      //     inlineMath: [['$','$'], ['\\(','\\)']]
      //   } 
      //   "HTML-CSS": {
      //     preferredFont: "STIX",
      //     matchFontHeight: true,
      //   }
      //   });
      MathJax.Hub.Config({
          extensions: ["tex2jax.js"],
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
          },
          "HTML-CSS": { 
            availableFonts: ["TeX"],
            matchFontHeight: true,
          }
      });
      
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
  </head>
  
  

  <body link="blue">
    
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/steevelaquitaine/projBrainInference">Github</a>
          <a id="project_author" href="http://steevelaquitaine.blogspot.com">Steeve laquitaine </a>
        </header>
    </div>

    <!--TITLE-->
    <div id="proj_title_wrap" class="outer">
        <header class="inner">
        <section id="proj_title" class="inner">
        <h1 id="proj_title"> How does the brain do visual inference ? </h1>
    </div>
    
    

    <!--Table of contents-->
    <div id="Table_of_content_wrap" class="outer">
      <section id="table_of_content" class="inner">

        <h5> TABLE OF CONTENT </h5>
        <h7><a href="#Inference"> INFERENCE <br></h7>
        <h7><a href="#Design-an-inference-experiment"> DESIGN AN INFERENCE EXPERIMENT <br></h7>
        <h7><a href="#Database"> DATABASE <br></h7>
        <h7><a href="#Data-preprocessing-workflow"> DATA PREPROCESSING WORKFLOW <br></h7>
        <h7><a href="#Mapping-brain-areas"> MAPPING BRAIN AREAS <br></h7>
        <h7><a href="#Look-at-fMRI-voxel-responses"> LOOK AT BRAIN RESPONSES  <br></h7>      
        <h7><a href="#Brain-decoding-of-stimulus-machine-learning"> BRAIN DECODING (FISHER LINEAR DISCRIMINANT-LOO) <br></h7>
        <h7><a href="#Motion-noise"> &nbsp &nbsp DECODE MOTION NOISE <br></h7>
        <h7><a href="#Motion-direction"> &nbsp &nbsp DECODE MOTION DIRECTION <br></h7>        
        <h7><a href="#switching"> &nbsp &nbsp ESTIMATION BEHAVIOR <br></h7>
        <h7><a href="#voxel-population-analysis"> &nbsp &nbsp BRAIN RESPONSE POPULATION ANALYSIS <br></h7>
        <h7><a href="#Brain-decoding-with-Channel-Encoding-reconstruction"> &nbsp &nbsp BRAIN DECODING WITH FORWARD MODELING RECONSTRUCTION <br></h7>
        <h7><a href="#Probabilistic-population-decoding"> &nbsp &nbsp BRAIN DECODING WITH PROBABILISTIC POPULATION MODELING <br></h7>
        <h7><a href="#Decoding-subjects-combined-datasets"> &nbsp &nbsp DECODING SUBJECTS COMBINED DATASETS <br></h7>
      </section>
    </div>
    
    
    
    
    
    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
      <h6><a id="Inference" class="anchor" href="#Inference" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Inference</h3>  
      <p></p>

      <h6><a id="Design-an-inference-experiment" class="anchor" href="#Design-an-inference-experiment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Designing an inference experiment</h6>
      <p>We designed a motion direction estimation experiment in which humans were asked to estimate the 
      motion direction of noisy stimuli on a computer screen. In this experiment statistical optimality 
      can be achieved by combining noisy evidence of the motion with knowledge of the motion direction 
      statistics learnt over motion stimulus history using Bayesian inference</p>
      
      <p> Subjects were first trained with one prior distribution then scanned with the same prior. 
      In subsequent session, subjects are trained with a new prior and scanned with this new prior. </p>
      <center><img src="images/experiment.png" style="width: 50%; height: 50%"/></center>
      
      <p> The line by line steps of the experiment are stored in the following matlab script : <p>
      <!--Codes-->
      <code class="code">
          >> run runAlltaskDirfMRI05.m
      </code>
      
      <h6><a id="Database" class="anchor" href="#Database" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Database</h3>
      <a href="dataset/datasetBrainInfer.csv"> Click for dataset info </a>

      <h6><a id="Data-preprocessing-workflow" class="anchor" href="#Data-preprocessing-workflow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data preprocessing workflow</h3>
      <Strong> In scanner behavioral data </Strong>  
      <p> The raw behavioral data we collected are saved in files under matlab format (".mat") in a project folder we called "sltaskDotDirfMRI05".
      Open matlab script "slworkflowBehData.m". It contains the steps to preprocess and organize the data in ".mat" file format ready  
      to be analyzed with the script "analyses.m". The subject id and its associated files for each condition are already set in "slworkflowBehData.m"  </p>
      
      <!--code-->
      <code class="code">
          > run slworkflowBehData.m <br>
          % analyse data from subject02 with prior at 225 deg <br>
          > cd p225 <br>
          > analyses({'sub02'},{'StimStrength','Pstd','FeatureSample'},'inpath','experiment','vonMisesPrior','dataDis','signDistance');<br>
      </code>
      
      <Strong> Brain activity data </Strong>  
      <p> Brain activity data (heavy files) were stored for each subject under the .niftii format. Each scan within each session collected was motion compensated with linear interpolation and drift correction using the mean frame
      (acquired volume) as base frame. Multiple sessions were collected for each prior conditions. The resulting fMRI images were filtered and high-passed with a filter cut-off
      of 0.01, then the intensity was normalized to percent signal change relative to the mean intensity. Scans were then concatenated over sessions. </p>
      
      <p> We then created a more operational database (easier to load/manipulate/use) database of Bold response instances to the displayed stimulus at each trials.
        Bold time series are extracted from the .niftii files for specified ROIs ("ROITSeries") and stored (faster load). The BOLD response instances for those ROI 
        and their associated task variables are saved as "d.instances", "d.myRandomDir" etc., in a matlab file "d.mat" at the path : 
        "o.stckPath 'slStckAnalyses/' o.myGroup '/classif/' 's' o.sid '/prior' num2str(o.priormean) '/' o.myVar '/' [o.myAnalysis{:}] '/' o.myROIname{:} '/']". The
        for example :
        "/data/datafMRI/sltaskdotdirfmri05/slStckAnalyses/Concatenation/classif/s323/ <br>
        prior225/myRandomCoh/accAtTimeleaveOneOutfisherbalancByRemovI/V1" following code is run for each subject, prior condition and visual roi.
      <p>
      
      <!--code-->
      <code class="code">
      > slfmriInitAnalysisTaskDotDirfMRI05 
      > d = slfmriGetDBoverSessions(o,varargin);         
      </code>  
        
      <h6><a id="Mapping-brain-areas" class="anchor" href="#Mapping-brain-areas" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mapping brain areas</h3>
      <p> The predominant view is that the brain is a collection of areas that are specialized for particular processing. e.g., The occipital cortex (largely involved in visual perception) has been divided into areas included V1, specialized in edge orientation processing, MT, specialized in motion processing etc...
      The first step consists in mapping those visual areas. We used retinotopic mapping to identify visual areas : 6-8 scans of bars swipping the visual screen.
      We then run a population receptive field analysis that identify voxels preferred spatial location (eccentricity and polar angle).
      The analysis consists in modeling voxels responses to different stimulus locations with a 2D Gaussian that accounts for the hemodynamic response
      timecourse, hemodynamic response timecourse was modelled with a difference of gamma function (fminsearch). <p>
        
      <p>We roughly divided the brain into parietal and occipital regions. To visualize those areas in mrTools load a surface (.off fiels) 
      produced by freesurfer, create an occipital/parietal flat map on that surface, load the flat map and create an roi
      out of the entire flat map, then display the roi on the surface). </p>

      <center><img src="images/left_parietal_occipital.png" style="width: 50%; height: 50%"/></center>

      <p> We then run an event-related analysis to map the areas that responded to motion (highlighted below in hot colors (yellow) on different views of an inflated brain </p>
      <center><img src="images/MotionBrain.png" style="width: 30%; height: 30%"/></center>
      
      <h6><a id="Look-at-fMRI-voxel-responses" class="anchor" href="#Look-at-fMRI-voxel-responses" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Look at fMRI voxel responses</h3>
          
      <!--code-->
      <code class="code">   
          >> slfmriInitAnalysisTaskDotDirfMRI05 <br>
          >> [d,o,behbySess] = slfmriGetDBoverSessions(o,varargin); <br>
          >> [~,VoxelsParams] = slfmriGetVoxTuningParams('vonMisesprior','neuralvector',d,o); <br>
          >> slfmriVisualDatabase(d.instances(d.mySwitch==1,:),'x1',d.myRandomDir(d.mySwitch==1,:),'x2',VoxelsParams.modes1); <br>
          >> slfmriVisualDatabase(d.instances(d.mySwitch==2,:),'x1',d.myRandomDir(d.mySwitch==2,:),'x2',VoxelsParams.modes2)
      </code>

      
      <h6><a id="Decode-stimulus-features-from-brain-activity" class="anchor" href="#Decode-stimulus-features-from-brain-activity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decode stimulus features from brain activity</h3>  
      
      <ul>
      <li> Are we able to decode prior conditions from any area? </li>
      <li> Are we able to decode likelihood conditions (motion directions, coherence) ? </li>
      </ul>


      <h6><a id="Motion-noise" class="anchor" href="#Motion-noise" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>DECODE MOTION NOISE</h3>  
      <p>We also decoded the 2 coherences from the voxels Bold pattern same brain regions. </p>
      
      <p> I have stored clean data (Nv voxels by Ni instances response matrices) for each roi and concatenated over experimental sessions for each subject.  You can directly run classification from these datasets. </p>      

      <!--Code-->
      <code class="code">      
            %load data, shape and classify <br>
            > load /Volumes/DroboBKUP/data/datafMRI/sltaskdotdirfmri05/... <br>
            &nbsp &nbsp &nbsp &nbsp        slStckAnalyses/Concatenation/classif/s25/prior225/myRandomCoh/... <br>
            &nbsp &nbsp &nbsp &nbsp        accAtTimeleaveOneOutfisherbalancByRemovI/V1/d.mat <br>
            > [M_struc,v_u] = slmat2structByVar(d.instances,d.myRandomCoh); <br>
            > c = leaveOneOut(M_struc,'balancByRemovI=1'); <br>
      </code>  

      <p> You can classify coherences from the data for all subjects like that : </p>      

      <!--Code-->
      <code class="code">      
      %set subjects as rows and classification output as column <br>
      > rows = {'s24','s25','s323','s327','s357'}; <br>
      > cols = {'correct','correctSTE'}; <br><br>

      > for i = 1 : length(rows) <br><br>
             
          &nbsp &nbsp %load data, shape and classify <br>
          &nbsp &nbsp > load(['~/data/datafMRI/sltaskdotdirfmri05/'... <br>
          &nbsp &nbsp &nbsp &nbsp    'slStckAnalyses/Concatenation/classif/' rows{i} '/prior225/myRandomCoh/'...<br>
          &nbsp &nbsp &nbsp &nbsp    'accAtTimeleaveOneOutfisherbalancByRemovI/IPS/d.mat'])    <br><br>
          &nbsp &nbsp %select variable to classify <br>
          &nbsp &nbsp > variable = d.myRandomCoh; <br><br>
        
          &nbsp &nbsp %balance classes by taking minimum common number of instances <br>
          &nbsp &nbsp > iCells = slsetNumInstancesByVar(d.instances,variable ,'balanceByRemovI'); <br><br>
        
          &nbsp &nbsp %Fisher classification <br>
          &nbsp &nbsp > c = leaveOneOut(iCells,'balancByRemovI=1');<br>
          &nbsp &nbsp > clear('iCells')<br><br>
         
          &nbsp &nbsp %store <br>
          &nbsp &nbsp > classifres(i,1) = c.correct;   <br>
          &nbsp &nbsp > classifres(i,2) = c.correctSTE;   <br><br>

      > end <br><br> 

      </code>  

      <p> Classification results for roi V1 are saved in <a href="dataset/datasetClassifCoh.csv"> dataset/datasetClassifCoh.csv file</a> by : </a> </p>

      <code class="code">  
          %convert result to .csv <br>
          > dataset = [cols; [rows' num2cell(dataClassif)]]; <br>
          > slconvMatAllCellsToCsv('datasetClassifCoh',dataset) <br>
      </code>  
        

      <p> You can also run the analysis directly from the raw data (permission is required to download from Stanford NIMS database 
      and the data needs to be preprocessed with the lab function "dofmricni.m")) </p>

      <!--Code-->
      <code class="code">      
           %First initialize the parameters as above for directions .. <br>
           %..then decode <br>
           >> slfmriClassifyMotionCoherence(o);
      </code>
      
      
      <h6><a id="Motion-direction" class="anchor" href="#Motion-direction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>DECODE DIRECTION</h3>  
      <p>We decoded the 5 motion directions from the voxels Bold pattern measured in 8 visual brain regions engaged in 
      processing motion information. </p>
      
      <!--Code-->
      <code class="code">
           %Initialize parameters <br>
            >> o = slfmriInitTask('Concatenation',1,2,1,2,o); <br>
            >> o = slfmriInitSession('~/data/datafMRI/sltaskdotdirfmri05/',... <br>
            &nbsp &nbsp &nbsp &nbsp    {'s02520150814','s02520150923','s02520150925'},o); <br>
            >> o = slfmriInitRois({'outsideBrain03','V1','V2','V3','V3A','MT','IPS','V1toMT'},...<br>
            &nbsp &nbsp &nbsp &nbsp   's0025_flatL_WM_occipital_Rad90',... <br>
            &nbsp &nbsp &nbsp &nbsp  '~/data/datafMRI/mlrAnatDB/s0025/mlrBaseAnatomies/',... <br>
            &nbsp &nbsp &nbsp &nbsp  '~/data/datafMRI/mlrAnatDB/s0025/mlrROIs/',o); <br><br>
          
          %plot <br>
          >> slfmriClassifyMotionDirections(o); 
      </code>
            
      <!--Section-->
      <h6><a id="switching" class="anchor" href="#switching" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Estimation behavior</h3>  
      <p>We also decoded when subjects switched to the prior or the likelihood from the voxels Bold pattern of the same brain regions. </p>
      
      <!--Code-->
      <code class="code">      
           %First initialize the parameters as above for directions .. <br>
           %..then decode <br>
           >> slfmriClassifySwitchingbyCoh(o);
      </code>

      <!--text-->
      <p><strong> Did the subject's brain maintained a representation of the displayed motion direction when the subject switched to his prior ? </strong>
      <p> We decoded the 4 motion directions (except at the prior mean, because those trials can be classified as switching to evidence or prior mean) displayed on each trial 
      from the activity patterns recorded from the subject s025's brain when he switched to his prior mean and when he switched to the 
      sensory evidence separately (at the weakest motion, 6% coherence, 3 sessions of 80 deg prior). The sample size was 32 trials/direction and 8 trials/direction when subjects 
      switched to sensory evidence and prior respectively).</p> 
      
      <!--code-->
      <code class="code">
       >> slfmriInitClassifAnalysisTaskDotDirfMRI05 <br>
       >> dataClassif = slfmriClassifyDir_x_switch(o)
      </code>
      
      <!--figure-->
      <center><img src="images/Classif4DirBySwitch.png" style="width: 70%; height: 70%"/></center>
      
      <p><strong> The classifier showed very poor decoding accuracies when trying to classify 5 motion directions. But it might have 
      a hard time identifying differences in patterns elicited by 5 motion directions.
      
      <!--subsection-->
      <p><strong> It is unfair to compare the classification accuracies produced from 32 trials in one condition with 
      the accuracy produced by classifying from 8 trials which should be poorer because the sample size is smaller. </strong> 
      We equalized the sample size between switching conditions to 8 instances (sampled out of 32 without 
      replacement) and rerun the classifications for 7 rois (V1 - V3, V3A, MT, IPS) and a control roi outside the brain. 
      Sampling was performed 100 times (minimum number of samples) from each class and the 100 resulting classification 
      accuracies were averaged for each conditions and roi with the 95% confidence interval calculated over bootstrapped 
      samples. </p> 
        
      <!--Code-->
      <code class="code">
        
          >> slfmriInitClassifAnalysisTaskDotDirfMRI05 <br>

          >> params = {'accuracyAtTime',[7 14; 7 14;7 14; 7 14;7 14; 7 14;7 14],... <br>
              &nbsp &nbsp &nbsp &nbsp       'loadSavedROI','CalcInstances','leaveOneOut','fisher',... <br>
              &nbsp &nbsp &nbsp &nbsp       'numInstances',8,'CalcInstancesByCond'};

          >> myConds = {{'myRandomDir_x_mySwitch=1_x_myRandomCoh=0.06'},.... <br>
              &nbsp &nbsp &nbsp &nbsp     {'myRandomDir_x_mySwitch=2_x_myRandomCoh=0.06'}}; <br>

          >> [stat,cbycByROI,sbycByROI,oByROI,obycByROI]=... <br>
              &nbsp &nbsp &nbsp &nbsp     slfmriClassifyBootInstByConditions(100,params,myConds,o);
              
      </code>
          
      <center><img src="images/classifAccEqSamplSizeByBoot.png" style="width: 50%; height: 50%"/></center>
          
      <!--results-->
      <p> None of the accuracies were significantly above chance. There might seem below chance because 
      Fisher leaveOneOut slightly bias predictions against the test set (not sure, it's an explanation for 
      svm but it should not bias classification for fisher) or because classification outputs are unstable for
      such small sample size (8 instances). </p>
      
      
      
      <!--subsection-->
      <p> Can he do better with 2 motion directions ? </strong> </p>
      <p> We chose the two directions furthest from the prior (225 deg in blue), where switching is the clearest: 15 and 85 degrees. 
      The classifier outputted one class at each of the 8/32 trials collected for each direction and when the subject switched to prior/evidence. 
      We report the classifier's percent correct prediction. </p> 
      <!--code-->
      <code class="code">
         >> drawVectors([15 85 155 225 295],[1 1 1 1 1])
      </code>
      
      <!--figure-->
      <center><img src="images/ClassifyTwoDirections.png" style="width: 100%; height: 100%"/></center>
      
      <p> We classified the two directions from various visually responsive areas of the brain (V1,V2,V3,V3A,hMT,IPS) 
      for subject "s025" over 3 sessions when motion and the prior were weak (6% coherence, 80 deg prior). We also 
      classified the directions from signals recorded outside the brain as a control check that significant accuracies were not 
      spurious (e.g., error in the code).
      
      <!--figure-->
      <center><img src="images/ControlROI.png" style="width: 50%; height: 50%"/></center>
      
      <!--code-->
     <code class="code">
              >> slfmriInitClassifAnalysisTaskDotDirfMRI05_loop; <br> 
              >> [accuracy, ste, dataClassifsw1, dataClassifsw2, nTrials] = slfmriClassifyTwoDir_x_switch_loop(o);
      </code>
      
      <!--figure-->
      <center><img src="images/ClassifTwoDir.png" style="width: 50%; height: 50%"/></center>
      
      <!--SECTION-->
      <h6><a id="Analysis-of-population-of-voxel-selectivity" class="anchor" href="#Analysis-of-population-of-voxel-selectivity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>VOXEL POPULATION ANALYSIS</h3>  

      <code class="code">
         >> run slfMRIwrapperDisVoxSelbalDir
      </code>

      <h7><a href="#Brain-decoding-with-Channel-Encoding-reconstruction"> &nbsp &nbsp BRAIN DECODING WITH FORWARD MODELING RECONSTRUCTION <br></h7>
      <h6><a id="Brain-decoding-with-Channel-Encoding-reconstruction" class="anchor" href="Brain-decoding-with-Channel-Encoding-reconstruction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>STIMULS RECONSTRUCTION WITH BRAIN RESPONSE FORWARD MODELING</h3>   


    
      <!--SECTION-->
      <h6><a id="Probabilistic-population-decoding" class="anchor" href="#Probabilistic-population-decoding" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PROBABILISTIC POPULATION DECODING</h3>   
      <p> We used a forward modeling approach (Brouwer & Heeger) that consists in modeling the brain voxel responses as a linear sum 
      of cosine functions (we could think of them as neural tuning functions to a stimulus feature) to reconstruct a stimulus feature
      (here the motion direction). </p>
        
      <!--Equations-->
      Each voxel $i$ response $b_i$ is modelled by : <br>
      
                                              \[ b_i = \sum_k^K W_{i_k}(f_k(s) + \eta_k) + \nu_i \]
      
      <p> where, <br>
      $i$ : voxel i <br>
      $s$ : stimulus direction <br>
      $W_{i_{k}}$ : contribution of population $k$ to the response of voxel $i$ <br>
      $f_k(s)$ : direction tuning functions (also called channels, half-wave rectified cosines raised to the power 5)
      of $K$ neural populations ($K$=8) tuned to different directions <br> </p>
      Tuning functions are defined as : <br>
      
                                            \[f_k(s) = max \Big(0,cos(\pi(s-\Phi_k)/180) \Big))^5\]
                                            
      where, <br>
      $\Phi_k$ : orientation preference of neural population $k$ <br>
      note that sine waves are defined as cos(AngFreq*space + phase) and take radians as inputs, 
      thus the phases are the orientation preferences $\Phi_k$ in degrees are $\Phi_k/180*pi$ in radians 
      and the period is $pi/180$ or 360 degs ($=2pi/360$). Ruben S van Bergen et al., 2015, Nature 
      Neuroscience use $cos(\pi(s-\Phi_k)/90)$ becausethe circular stimulus feature they manipulate is 
      orientation in which case sine wave periods are 180 degs. </p>
      
      <p> Responses are varible due to multiple sources of noise : <br> 
      $\eta_k$ and $\nu_i$ are deviations (noise) from mean response of channel $k$ and mean voxel $i$ responses respectively. <br>
      They are modelled as follows : <br>
      $\eta \sim \mathcal{N}(0,\sigma^2I)$ : 1 x K vector of noise or deviation from the mean response of each channel $k$ shared among neural populations of similar tunings 
      , generated by a Gaussian process with mean 0 and square diagonal covariance matrix $\sigma^2I$  <br><br>
      $\nu \sim \mathcal{N}(0,\Sigma)$ : 1 x M voxel vector of noise generated by a Gaussian process with mean 0 covariance matrix $\Sigma$
      that is the linear combination of two sources of noise with covariance matrices: <br>
      - 1) $\rho\tau\tau^T$ the covariance matrix (square symmetric matrix of M x M voxels) that defines the noise specific to individual voxels $i$ which contribution to $\nu$ is scaled by $\rho$ <br>
      , where $\tau$ is a 1 x M vector that models the std $\tau_i$ of each voxel $i$ noise <br>
      - 2) $(1-\rho)I\circ\tau\tau^T$ the covariance matrix (diagonal square matrix of M x M voxels) that defines the noise shared among voxels irrespective of their tunings
      (common deviation from voxels mean response scaled by $1-\rho$) <br> 
      $\tau$ : a vector of 1 x M voxels that models the std $\tau_i$ of each voxel's noise <br>
      $\rho$ : 1 scalar factor modeling the contribution of the voxel-specific noise and the noise shared globally among voxels irrespective of their tunings <br><br>
      $\Sigma = \rho\tau\tau^T + (1-\rho)I\circ\tau\tau^T$  <br> </p>
          
       
      <p>The idea is to search the model parameters ($W$,$\rho$,$\sigma$,$\tau$) that maximize the fit between actual and predicted bold patterns.<br>
      Model fitting was done in two steps : <br>   
      - On a training data set, find $W$ by assuming that $\sigma=0$ which simplifies the fit to least-square regression :  <br>
                                                
                                                \[ \hat{W}_i = b_i f(s)^T(f(s)f(s)^T)^-1 \]
      
      because : <br>
      \[ b_i = \sum_k^K W_{i_k}(f_k(s) + \eta_k) + \nu_i \]
      becomes : <br>
      \[ b_i = \sum_k^K W_{i_k}(f_k(s)) + \nu_i \] where $W_{i_k}$ are the weights of the linear regression and $\nu_i$ is the Gaussian residual.
      </p>
          
      Once we get the weights, the seconds step is to find $\rho$,$\sigma$,$\tau$ given those weights are fixed using maximum likelihood fit.
      The likelihood of each voxel response pattern is given by the generative model : <br>
      
      \[ p(b|s,\eta;W,\Sigma) = 1/\sqrt(2\pi |\Sigma|) exp( -1/2(b-W(f(s)+\eta))^T \Sigma^-1(b-W(f(s) + \eta)) ) \]
      
      marginalizing over $\eta$ : <br>
      
      \[ p(b|s,W,\Omega) = \int p(b|\eta,s;\Sigma)p(\eta)d\eta =  1/\sqrt(2\pi|\Omega|) exp( -1/2(b-Wf(s))^T \Omega^-1(b-Wf(s))  ) \] 
      
      where, <br>
      
      \[ \Omega = \rho\tau\tau^T + (1-\rho)I\circ\tau\tau^T + \sigma^2WW^T \] 
    
    
    <p> <strong> To develop the code and make sure that everything works as expected, let's first simulate some voxel response
    to several trial instances of displayed stimulus directions produced by the generative model </strong>; then we'll infer the posterior probabilities of those 
    probability of the directions give the voxel response patterns at each trial. If everything is ok, the posterior circular mean
    should more or less match the displayed directions (slight deviations are expected due to noise).  </p>
    
    <p> The number of voxels is large compared to the number of instances e.g., 320 voxels for V1 compared to about 240 trials. That might 
    affect our ability to train the weights with Ordinary least squares if lots of voxels have correlated responses (matrix columns are 
    linearly dependent and thus there are more than one set of weights that can solve the regression). So we first tried to reduce the number
    of voxels by selecting the more responsive to the motion stimulus (in the localizer, not the actual task which would cause sharing of
    information between training and test sets).</p> 
    
    <p> We get the r-squared of fitting the voxel responses to a periodic motion stimulus (localizer) with a sinewave with the period as 
    the stimulus. The voxels with very high r-squared, the well fitted ones respond in a way that is correlated with the appearance of the
    motion stimulus and are thus considered motion responsive : </p>
      <code class="code">
              >> sesspath = '~/data/datafMRI/sltaskdotdirfmri05/s02520150814/'		
              >> group = 'Averages';
              >> roipath = '~/data/datafMRI/mlrAnatDB/s0025/mlrROIs/V1.mat';
              >> [r2,o] = slfmriGetMotLocr2(roipath,sesspath,group);
      </code>
    
    <p> Now what we need is a response matrix $b$ of Ni instances x Nv voxels and its associated vector of displayed motion directions "svec". 
    We will run a cross-validated reconstruction of the likelihood $p(b|s,\rho;\tau,\sigma)$ of the responses given the model and hypothetical 
    motion directions. This analysis consists in dividing the response matrix into 5 folds, train the model on the 5 combinations of 4/5 folds
    then test on the remaining fold. We then look at the average results over folds : </p>
    
    <p> We get the instance matrix $b$ and its associated direction vector $svec$ </p>
    <code class="code">
          >> slfmriInitAnalysisTaskDotDirfMRI05 <br>
          >> d = slfmriGetDBoverSessions(o,varargin); <br> 
          >> b = d.instances; svec = d.myRandomDir;
    </code>
      
    <p> We decode the likelihood of hypothetical motion directions given the voxel responses and the model </p>
    <code class="code">
       [LLH_f,pp] = slvoxppKFoldCVdec(b,svec,5)
    </code>
    
    
    <!--SECTION-->
    <h6><a id="Decoding-subjects-combined-datasets" class="anchor" href="#Decoding-subjects-combined-datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>DECODING SUBJECTS COMBINED DATASETS</h3>   
    
    <p> <strong> We need lots of data to train the model well. </strong>  The more instances in the weight matrix (matrix row) and the more likely it is that the 
    is that each voxel data, the matrix columns are linearly independent (weight matrix columns), and that there is a unique vector of weights
    solution to the linear regression in the first step of the model training. So we want lots of data. </p>
    
    <p> <strong> Combine datasets between subjects </strong> One way to do that is to combined data across subjects. We collected data from 5 subjects. The issue is that an visual rois are not identical between subjects
    e.g., V1 does not have the same number of voxels (datasets have different number of features) in subject 1 and 2. Another issue is that 
    certain subjects missed reporting estimates in all trials which results in a different number of instances between subjects datasets (datasets have 
    different number of observations). Thus how to combine datasets with different number of features and observations. </p>
        
    <p> <a href="http://gru.stanford.edu/doku.php/mrTools/talairachCoordinates">Anatomical alignment ? </a> functional voxel matching ? Hyperalignment ? http://haxbylab.dartmouth.edu/publications/hyperalignment_cns2010_JSGuntupalli_JVHaxby.pdf.
      Based on a preliminary study from Haxby et al., Classification of datasets aligned over subjects show lower accuracy than within subject 
      when anatomically aligned, and at best the same accuracy when functional hyperalignment is used. So Alignment between subjects is not our 
      a solution to improve classification accuracy. So we collected lots of data for each subject which will improve the model training and thus
      its accuracy. /p>   

    <p></p>
    
    </div>
    
    
    
    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">projBrainInference maintained by <a href="https://github.com/steevelaquitaine">steevelaquitaine</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

  </body>
</html>
